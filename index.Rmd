---
title: "Practical Machine Learning Course Project"
author: "Ahmed Shakarchi"
date: "October 22, 2018"
output: html_document
---


##Load required libraries
```{r message= FALSE, warning = FALSE, error = FALSE, results = 'hold'}
library(tidyverse)
library(caret)
library(forcats)
```

##Read in dataset
```{r message= FALSE, warning = FALSE, error = FALSE, results = 'hold'}
data <- read_csv("pml-training.csv")
the_test <- read_csv("pml-testing.csv")
```


##Split dataset
```{r}
#60% training, 20% testing, 20% validation
set.seed(81719)
inTrain <- createDataPartition(data$classe, p = 0.6, list = FALSE)
training <- data[inTrain,]
non_training <- data[-inTrain,]
set.seed(9825)
inTest <- createDataPartition(non_training$classe, p = 0.5, list = FALSE)
testing <- non_training[inTest,]
validation <- non_training[-inTest,]
```


##Exploratory analysis
```{r}
glimpse(data)
View(data[1:50,])
```

```{r}
table(data$classe)

table(data$user_name)
table(the_test$user_name)
```

Same users in training and test dataset ... so use **user_name** to predict! There could be important differences between individuals on how they perform a particular **classe** of biceps curl, which we can use for prediction. If this algorithm were to be ported for widespread use, then **user_name** would most likely need to be omitted from the algorithm depending on application.



The description of the dataset, on http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har, indicates that the six subjects were asked to do 10 repetitions each. The reading from the body-worn sensors may thus overlap, and multiple data entries could cover the same repetition. Analyzing the information from the sensors with respect to time can give more accuracy in predicting the **classe** of biceps curl an individual subject was doing at a time; however, this will not be used for the current analysis. This is because the prediction problem in the test dataset requires us to predict over random single observations rather than a chuck of contiguous readings. Additionally, it appears that the observations are already obtained by taking into account readings from the sensors over a window of a specific length of time. The variable **num_window** is probably used to identify the ordering of these readings.

```{r}
table(data$num_window)
ggplot(training, aes(x = classe, y = num_window)) + 
  geom_boxplot()
qplot(X1, num_window, color = user_name, data = training)

#how to identify individual exercises?
data %>% filter(new_window == "yes") %>% nrow()
#406 new_window in data
```

Perhaps **new_window** identifies a new repetition in the dataset; however, this is unlikely because there are 6 subjects with 10 repetitions each. We would expect 60 yes's if that were the case. Regardless, we can omit looking longitudinally at multiple observations as it is not required for our current prediction problem. 


```{r}
map_dbl(training, function(x) {
  mean(is.na(x))
}) %>%
  knitr::kable()
```

It looks like the missing variables are only there when **new_window** is \"no\", indicating that these variables are averages, or other form of aggregate measure, for a particular repetition.


```{r}
var_missing <- map_dbl(training, function(x) {
  mean(is.na(x))
})

training1 <- training %>% 
  select(-which(var_missing > 0)) %>%
  select(-c(X1, new_window, num_window, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp))

#set categorical variables are factors
training1 <- training1 %>%
  mutate(classe = as_factor(classe),
         user_name = as_factor(user_name))

```


The variable **X1** in the training dataset, which is also there in the test dataset, is merely a row-number variable. It would need to be removed, as any association it may have with the **classe** of exercise is likely to be coincidental, because of an association between **classe** and the variable used to sort the dataset. Additionally, readings obtained around the same time as each other are likely to reflect the same exercise or different repetitions of the same exercise. It would be prudent, thus, to remove these variable from our predictive model. **num_window** and **new_window** are also removed for the reasons stated above. Finally, variables with missingingness, mostly likely aggregate measures for a particular repetition, were also removed.


##Apply same data manipulations to testing and validation sets
```{r}
testing1 <- testing %>% 
  select(-which(var_missing > 0)) %>%
  select(-c(X1, new_window, num_window, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp))
testing1 <- testing1 %>%
  mutate(classe = as_factor(classe),
         user_name = as_factor(user_name))

validation1 <- validation %>% 
  select(-which(var_missing > 0)) %>%
  select(-c(X1, new_window, num_window, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp))
validation1 <- validation1 %>%
  mutate(classe = as_factor(classe),
         user_name = as_factor(user_name))
```


##Model building 
```{r eval = FALSE}
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

model_rpart <- train(classe ~ ., data = training1, method = "rpart", trControl = trctrl, preProcess = c("center", "scale"))
model_svm <- train(classe ~ ., data = training1, method = "svmLinear", trControl = trctrl, preProcess = c("center", "scale"))
model_lda <- train(classe ~ ., data = training1, method = "lda", trControl = trctrl, preProcess = c("center", "scale"))

model_gbm <- train(classe ~ ., data = training1, method = "gbm", preProcess = c("center", "scale"))
model_rf <- train(classe ~ ., data = training1, method = "rf", preProcess = c("center", "scale"))
```

I set **eval** = FALSE for the above chunk, because the random forest and gradient boosting machine took a long time to run when I initially ran the model. Instead of running the code, I will load the environment from I ran the models initially. 
```{r}
load(".RData")
```


```{r}
pred_rpart <- predict(model_rpart, newdata = testing1)
pred_svm <- predict(model_svm, newdata = testing1)
pred_lda <- predict(model_lda, newdata = testing1)

pred_gbm <- predict(model_gbm, newdata = testing1)
pred_rf <- predict(model_rf, newdata = testing1)


confusionMatrix(pred_rpart, testing1$classe) #0.49
confusionMatrix(pred_svm, testing1$classe)   #0.75
confusionMatrix(pred_lda, testing1$classe)   #0.6689

confusionMatrix(pred_gbm, testing1$classe)   #0.9551
confusionMatrix(pred_rf, testing1$classe)    #0.9878
```

Testing accuracy on the testing dataset shows that the RF model has the highest accuracy (98.8%). It also has excellent discrimination (Kappa = 0.98). Gradient Boosting Machine faired very well (accuracy = 96%, Kappa = 0.94), but not as good as the RF.

I will try stacking all 5 models using a random forest. This will probably not improve prediction by much given the very high accuracy of the random forest.
```{r}
stackedDF_training1 <- tibble(
  pred_rpart = predict(model_rpart, newdata = training1),
  pred_svm = predict(model_svm, newdata = training1),
  pred_lda = predict(model_lda, newdata = training1),
  pred_gbm = predict(model_gbm, newdata = training1),
  pred_rf = predict(model_rf, newdata = training1),
  classe = training1$classe
)
#model_stacked <- train(classe ~ ., data = stackedDF_training1, method = "rf")
#model had run & loaded from previous environment. Took a couple of minutes to run
```

```{r}
pred_stacked <- predict(model_stacked, newdata = tibble(
  pred_rpart, pred_svm, pred_lda, pred_gbm, pred_rf
))
confusionMatrix(pred_stacked, testing1$classe)  #0.9878

table(pred_rf, pred_stacked)
```
As expected, stacking the models does not improve accuracy. In fact, if we look in the table above, we see that the stacked model had exactly the same predictions as the random forest. 


Thus, I will choose the random forest as my final model. To have an idea about out-of-sample accuracy, I will test the model one final time in the validation set.
```{r}
pred_rf_val <- predict(model_rf, newdata = validation1)
confusionMatrix(pred_rf_val, validation1$classe)         #0.9918
```
Very high accuracy is seen in the validation set, which had not been used for model building or selection, and would, thus, approximate out-of-sample accuracy.



##Apply final model to quiz dataset
```{r}
the_test1 <- the_test %>% 
  select(-which(var_missing > 0)) %>%
  select(-c(X1, new_window, num_window, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp))
the_test1 <- the_test1 %>%
  mutate(user_name = as_factor(user_name))

pred_the_test_rf <- predict(model_rf, newdata = the_test1)
df <- tibble(id = the_test1$problem_id, 
             prediction = pred_the_test_rf)

df %>% knitr::kable()
```


